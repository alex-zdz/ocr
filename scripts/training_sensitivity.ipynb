{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell jupyter notebook to reload all modules before running each cell,thereby if \n",
    "# a function is modified, the changes are reflected immediately\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alex/projects/ocr/scripts\n",
      "/home/alex/projects/ocr\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Sensitivity Analysis\n",
    "# This notebook explores the sensitivity of the model's performance to various hyperparameters using a grid search approach.\n",
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "from src.train_test import *\n",
    "from src.train_test import  run_inference_and_calculate_cer\n",
    "from src.qwen_finetune import train_and_validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset_p40 = load_from_disk(\"data/processed/dataset_p40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-5, 3e-5, 5e-5]\n",
    "batch_sizes = [1]\n",
    "train_select_end = len(dataset_p40)\n",
    "accumulation_steps = [1, 2]\n",
    "image_factors = [28]\n",
    "max_steps_list = [5]\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_grid = list(itertools.product(learning_rates, batch_sizes, accumulation_steps, image_factors, max_steps_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the first set of parameters\n",
    "params = param_grid[0]\n",
    "lr, batch_size, acc_steps, img_factor, max_steps = params\n",
    "# Set up output directory with modified learning rate format\n",
    "output_dir = f'results/models/lr_{lr:.0e}'.replace('-', '_').replace('e-', 'e_') + f'_bs_{batch_size}_acc_{acc_steps}_img_{img_factor}_max_{max_steps}'\n",
    "\n",
    "output_dir\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MiB\n",
      "Max memory allocated: 0.00 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:391: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MiB\")\n",
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1024 ** 2:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1e-05, 1, 1, 28, 1000, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = param_grid[0]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, batch_size, acc_steps, img_factor,  max_steps = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Train model\n",
    "try:\n",
    "    train_and_validate(\n",
    "        model_name='Qwen/Qwen2-VL-2B-Instruct',\n",
    "        output_dir=output_dir,\n",
    "        dataset_name='culturalheritagenus/Gongguan-OCR-p40',\n",
    "        image_column='image',\n",
    "        text_column='text',\n",
    "        device='cuda:0' ,\n",
    "        min_pixel=256,\n",
    "        max_pixel=384,\n",
    "        image_factor=img_factor,\n",
    "        num_accumulation_steps=2,\n",
    "        max_steps=10,\n",
    "        train_select_end = 10,\n",
    "        train_batch_size=batch_size,\n",
    "        val_batch_size=1,\n",
    "        lr = lr\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "message = \"Convert this image to text\"\n",
    "small_test_dataset = dataset_p40.select(range(10))\n",
    "# Evaluate model\n",
    "df_results = run_inference_and_calculate_cer(f\"{output_dir}/final\", message, small_test_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate metrics\n",
    "mean_cer = df_results['CER'].mean()\n",
    "median_cer = df_results['CER'].median()\n",
    "memory_usage = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "#speed = max_steps / (time.time() - start_time)\n",
    "\n",
    "print(f\"Mean CER: {mean_cer}\")\n",
    "print(f\"Median CER: {median_cer}\")\n",
    "print(f\"Memory Usage: {memory_usage}\")\n",
    "#print(f\"Speed: {speed} steps/sec\")\n",
    "df_results['CER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train and evaluate a model with given hyperparameters\n",
    "def train_and_evaluate(params, message, dataset, output_path):\n",
    "\n",
    "    lr, batch_size, acc_steps, img_factor, train_select_end, max_steps = params\n",
    "    #output_dir = f'models/lr_{lr}_bs_{batch_size}_acc_{acc_    test_name = f'lr_{lrteps}_range_{train_start}_{train_end}_img_{img_factor}_eval_{eval_steps}_max_{max_steps}'\n",
    "    test_name = f'lr_{lr:.0e}'.replace('-', '_').replace('e-', 'e_') + f'_bs_{batch_size}_acc_{acc_steps}_trainend_{train_select_end}_img_{img_factor}_max_{max_steps}'\n",
    "    output_dir = f'{output_path}/{test_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # start timer\n",
    "    start_time = time.time() \n",
    "    \n",
    "    # Train model\n",
    "    train_and_validate(\n",
    "        model_name='Qwen/Qwen2-VL-2B-Instruct',\n",
    "        output_dir=output_dir,\n",
    "        dataset_name='culturalheritagenus/Gongguan-OCR-p40',\n",
    "        image_column='image',\n",
    "        text_column='text',\n",
    "        device='cuda:0', \n",
    "        min_pixel=256,\n",
    "        max_pixel=384,\n",
    "        image_factor=img_factor,\n",
    "        num_accumulation_steps=acc_steps,\n",
    "        train_select_end = train_select_end,\n",
    "        max_steps=max_steps,\n",
    "        train_batch_size=batch_size,\n",
    "        val_batch_size=1,\n",
    "        lr = lr\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    model_path = f\"{output_dir}/final\"\n",
    "    results = run_inference_and_calculate_cer(model_path, message, dataset)\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    results.to_csv(f'results/predictions/{test_name}.csv', index=False)\n",
    "\n",
    "    # Calculate metrics\n",
    "    mean_cer = results['CER'].mean()\n",
    "    median_cer = results['CER'].median()\n",
    "    memory_usage = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0\n",
    "    speed = (time.time() - start_time)\n",
    "    \n",
    "    return {\n",
    "        'params': params,\n",
    "        'mean_cer': mean_cer,\n",
    "        'median_cer': median_cer,\n",
    "        'memory_usage': memory_usage,\n",
    "        \"speed\": speed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try for one row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Convert this image to text\"\n",
    "output_path = \"results/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = dataset_p40.select(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(params, message, small_test_dataset, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete old model, processor and inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors on GPU larger than 10MB:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1113: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_668393/2360521219.py:6: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "def list_tensors_on_gpu(min_size_mb=10):\n",
    "    print(f\"Tensors on GPU larger than {min_size_mb}MB:\\n\")\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "                if obj.is_cuda:\n",
    "                    size_mb = obj.element_size() * obj.nelement() / 1024**2\n",
    "                    if size_mb > min_size_mb:\n",
    "                        print(f\"Type: {type(obj)}, Size: {size_mb:.2f} MB, Shape: {tuple(obj.shape)}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "list_tensors_on_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Run garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Step 2: Empty the PyTorch CUDA cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 0.00 MiB\n",
      "Max memory allocated: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()\n",
    "print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MiB\")\n",
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1024 ** 2:.2f} MiB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try doing it the \"old\" way with for loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "learning_rates = [1e-3, 1e-5, 1e-7] # \n",
    "batch_sizes = [1, 2]\n",
    "accumulation_steps = [1, 5]\n",
    "image_factors = [28]\n",
    "train_select_end = [round(len(dataset_p40) / 2), len(dataset_p40)]\n",
    "max_steps_list = [100000]\n",
    "\n",
    "# Create combinations of hyperparameters\n",
    "param_grid = list(itertools.product(learning_rates, batch_sizes, accumulation_steps, image_factors, train_select_end, max_steps_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.001, 1, 1, 28, 330, 100000),\n",
       " (0.001, 1, 1, 28, 661, 100000),\n",
       " (0.001, 1, 5, 28, 330, 100000),\n",
       " (0.001, 1, 5, 28, 661, 100000),\n",
       " (0.001, 2, 1, 28, 330, 100000),\n",
       " (0.001, 2, 1, 28, 661, 100000),\n",
       " (0.001, 2, 5, 28, 330, 100000),\n",
       " (0.001, 2, 5, 28, 661, 100000),\n",
       " (1e-05, 1, 1, 28, 330, 100000),\n",
       " (1e-05, 1, 1, 28, 661, 100000),\n",
       " (1e-05, 1, 5, 28, 330, 100000),\n",
       " (1e-05, 1, 5, 28, 661, 100000),\n",
       " (1e-05, 2, 1, 28, 330, 100000),\n",
       " (1e-05, 2, 1, 28, 661, 100000),\n",
       " (1e-05, 2, 5, 28, 330, 100000),\n",
       " (1e-05, 2, 5, 28, 661, 100000),\n",
       " (1e-07, 1, 1, 28, 330, 100000),\n",
       " (1e-07, 1, 1, 28, 661, 100000),\n",
       " (1e-07, 1, 5, 28, 330, 100000),\n",
       " (1e-07, 1, 5, 28, 661, 100000),\n",
       " (1e-07, 2, 1, 28, 330, 100000),\n",
       " (1e-07, 2, 1, 28, 661, 100000),\n",
       " (1e-07, 2, 5, 28, 330, 100000),\n",
       " (1e-07, 2, 5, 28, 661, 100000)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting line  0 out of 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.42it/s]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Training: 100%|██████████| 100000/100000 [7:54:27<00:00,  3.51it/s, loss=1.38]   \n",
      "/pytorch/aten/src/ATen/native/cuda/TensorCompare.cu:110: _assert_async_cuda_kernel: block: [0,0,0], thread: [0,0,0] Assertion `probability tensor contains either `inf`, `nan` or element < 0` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, params \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(param_grid):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstarting line \u001b[39m\u001b[38;5;124m\"\u001b[39m,i,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout of\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(param_grid))\n\u001b[1;32m      5\u001b[0m     all_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: params,\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_p40\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[7], line 34\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(params, message, dataset, output_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m     33\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 34\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference_and_calculate_cer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save predictions to CSV\u001b[39;00m\n\u001b[1;32m     37\u001b[0m results\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/predictions/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/projects/ocr/src/train_test.py:173\u001b[0m, in \u001b[0;36mrun_inference_and_calculate_cer\u001b[0;34m(model_path, message, dataset)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Run inference on the dataset\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m--> 173\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     cer \u001b[38;5;241m=\u001b[39m compute_CER(item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], prediction, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_lines\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_lines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCER\u001b[39m\u001b[38;5;124m\"\u001b[39m: cer\n\u001b[1;32m    181\u001b[0m     })\n",
      "File \u001b[0;32m~/projects/ocr/src/train_test.py:25\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image, model, messages_template, processor)\u001b[0m\n\u001b[1;32m     17\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m     18\u001b[0m     text\u001b[38;5;241m=\u001b[39m[text],\n\u001b[1;32m     19\u001b[0m     images\u001b[38;5;241m=\u001b[39mimage_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m generated_ids_trimmed \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     27\u001b[0m     out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids):] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     29\u001b[0m output_text \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     30\u001b[0m     generated_ids_trimmed, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2452\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2445\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2446\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2447\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2448\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2449\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2463\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2464\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2465\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2466\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2467\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2468\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2469\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:3463\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3462\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3463\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3465\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "for i, params in enumerate(param_grid):\n",
    "    print(\"starting line \",i,\"out of\", len(param_grid))\n",
    "    \n",
    "    all_results.append({\n",
    "    'params': params,\n",
    "    'results': train_and_evaluate(params, message, dataset_p40, output_path)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.0742416381835938e-05 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(f\"Time taken: {time.time() - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6129380861918134"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[0][\"results\"][\"speed\"] / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[456.7762851715088, 484.6063802242279, 319.8828670978546, 503.1054666042328]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeds = [r['results']['speed'] for r in all_results]\n",
    "speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_names, r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m all_results\n\u001b[1;32m      3\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[0;32m----> 2\u001b[0m     {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[43mparam_names\u001b[49m, r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m])), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mr[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m all_results\n\u001b[1;32m      3\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'param_names' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    {**dict(zip(param_names, r['params'])), **r['results']} for r in all_results\n",
    "])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
